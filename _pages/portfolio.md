---
layout: archive
title: "Portfolio"
permalink: /portfolio/
author_profile: true
---
Here are a complation of my projects and research from high school and college.
## Research
---
### üõ∞Ô∏è Quantitative Portfolio Management for NASA
*Sep 2022-May 2025* \
Continuation of undergraduate research to MEng thesis in Modern Portfolio Theory (MPT). Leveraging data science techniques to determine investments for technology projects. Strategizing initiatives to automate dynamic portfolio allocation for 10-20 year-long technology projects by improving upon MPT methodology and incorporating reinforcement learning frameworks. 
[[**Code**]](https://github.com/rwxhuang/astra-mpt-dashboard) [[**Website**]](https://mit-astra.streamlit.app/)

### üñ® CSAIL "Invisible" Ink Computer Vision Research
*Sep 2023 - Dec 2025* \
Assisted the development of hybrid paper documents for interactive digital media by leveraging AR techniques and computer vision models, resulting in 80%+ accuracy in detecting "invisible" QR codes; paper published/presented at CHI 2025. \
[[**Lab**]](https://hcie.csail.mit.edu/) [[**Paper**]](https://arxiv.org/abs/2502.17089)

### üìà Empirical Bayes using Deep Neural Networks: g-modeling vs f-modeling 
*Sep 2021 - Mar 2022* \
Researched empirical Bayes estimation via two main modeling strategies (g-modeling and f-modeling), and developing methodology to implement the above strategies using deep neural networks. Presented on Glow, a normalizing flow-based generative model with invertible 1x1 convolutions. \
 [[**Presentation**]](/files/glow_presentation.pdf)

## Projects
---
### üçÉ Social Media App for NYC Carbon Emissions
*April 2024* \
As our submission to the Point 72 Data Hackathon, we utilize NYC/Manhattan's Citi Bike data and Point 72's data streaming library CSP to create a communal app for users to update live how much carbon emission they are saving. \
[[**Code**]](https://github.com/Point72/csp)

### ‚åöÔ∏è LSTM vs Transformers for Time Series Modeling
*Sep 2023 - Dec 2023* \
A comparison analysis between LSTM and Transformer models in the context of time-series forecasting. In this study, we pinpoint which particular features of time series datasets could lead transformer-based models to outperform LSTM models. \
[[**Blog**]](https://deep-learning-mit.github.io/staging/blog/2023/time-series-lstm-transformer/)
[[**Code**]](https://github.com/rwxhuang/lstm_vs_transformers)

### üîä Predicting Volume from Silent Videos
*Feb 2023 - May 2023* \
From movies to video games, sound plays a crucial role in our perception of the world. In this project, we aim to predict the volume of visually indicated sounds (VIS) from silent video scenes. We present a model involving a recurrent-neural network composed of a CNN and an LSTM. \
[[**Paper**]](/files/comp_vision_final_project.pdf)

### ü•ó NutritionChecker
*Jul 2020* \
Using OCR, we design an Android application to scan nutrition labels and determine the healthiness of a grocery shopping trip. \
[[**Submission**]](https://devpost.com/software/nutritionchecker)
[[**Code**]](https://github.com/rwxhuang/NutritionChecker)
